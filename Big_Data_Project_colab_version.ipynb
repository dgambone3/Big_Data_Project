{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgambone3/Big_Data_Project/blob/main/Big_Data_Project_colab_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNXI8usqnKzW",
        "outputId": "5df391c0-beaf-4587-a4a8-b6f00938433d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425345 sha256=52eb92fb8b222ea750c1eff9ea89bfb0a1db80cff7789c06a18e80779b22b346\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdAUNoabnRhN",
        "outputId": "54cb03a1-7397-417a-b5bd-a07c73fe5dd5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "H-3QapShnJ-t"
      },
      "outputs": [],
      "source": [
        "import pyspark.sql.functions as sf\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.ml.feature import StopWordsRemover\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pyspark\n",
        "from pyspark.sql.functions import explode, sum\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WBtqfMVJnJ-v"
      },
      "outputs": [],
      "source": [
        "# from pyspark.sql import SparkSession\n",
        "\n",
        "# spark = (\n",
        "#     SparkSession.builder\n",
        "#     .master(\"spark://127.0.0.1:7077\")\n",
        "#     # the number of executors this job needs\n",
        "#     .config(\"spark.executor.instances\", 2)\n",
        "#     # the number of CPU cores memory this needs from the executor,\n",
        "#     # it would be reserved on the worker\n",
        "#     .config(\"spark.executor.cores\", \"2\")\n",
        "#     .config(\"spark.executor.memory\", \"4G\")\n",
        "#     .getOrCreate()\n",
        "# )\n",
        "# sc = spark.sparkContext\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a Spark session\n",
        "spark = SparkSession.builder.appName(\"example\").getOrCreate()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUt_bA35nJ-x"
      },
      "source": [
        "# Pet Supplies Product Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XxX-vOX_nJ-x"
      },
      "outputs": [],
      "source": [
        "data_df = spark.read.json(path='/content/drive/MyDrive/Big_Data/Data/Pet_Supplies.json')\n",
        "# data_df = pyspark.read.json(\"data/Pet_Supplies.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Data Stuff"
      ],
      "metadata": {
        "id": "zQ8HUhYFCX2k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vCOp2-Y3nJ-z"
      },
      "outputs": [],
      "source": [
        "# output count of reviewerID and count unique reviewerID\n",
        "print(data_df.select(\"reviewerID\").count())\n",
        "print(data_df.select(\"reviewerID\").distinct().count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzOGFbyQnJ-z"
      },
      "source": [
        "Add primary key to dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "OzXUyyy_nJ-0"
      },
      "outputs": [],
      "source": [
        "# add a distinct key to each row of data_df\n",
        "add_keys = data_df.withColumn(\"id\", sf.monotonically_increasing_id())\n",
        "\n",
        "#check count distinct id\n",
        "print(f\"Distinct keys: {add_keys.select(\"id\").distinct().count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yq7bpHrPnJ-0"
      },
      "source": [
        "## Process Review Text\n",
        "#### Preprocessing: remove null values for review text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmPu75dxnJ-0"
      },
      "outputs": [],
      "source": [
        "review_df = add_keys.select('id', 'overall', 'reviewText')\n",
        "\n",
        "# check null values\n",
        "print(f\"Null reviewText: {review_df.filter(sf.col('reviewText').isNull()).count()}\")\n",
        "print(f\"Null overall: {review_df.filter(sf.col('overall').isNull()).count()}\")\n",
        "\n",
        "# drop null reviewText values\n",
        "review_df = review_df.filter(sf.col('reviewText').isNotNull())\n",
        "\n",
        "# check for null values in review_df\n",
        "print(f\"Check null reviewText removed: {review_df.filter(sf.col('reviewText').isNull()).count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZAO4nOYnJ-1"
      },
      "outputs": [],
      "source": [
        "# calculate statistics of overall\n",
        "print(\"Statistics of overall rating: \")\n",
        "review_df.select('overall').describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GufQLYFInJ-1"
      },
      "source": [
        "#### Down sample dataset with matchng distribution for development due to large dataset size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ko50DMFrnJ-1",
        "outputId": "be57a0de-5545-4031-8d28-5ed55eb33877"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+--------------------+\n",
            "| id|overall|          reviewText|\n",
            "+---+-------+--------------------+\n",
            "|  7|    3.0|Maybe it's just m...|\n",
            "| 16|    1.0|Horrible just a w...|\n",
            "| 18|    5.0|           its great|\n",
            "| 50|    1.0|seriously, buy a ...|\n",
            "| 58|    5.0|This is the best ...|\n",
            "| 63|    1.0|When I bought thi...|\n",
            "| 66|    3.0|They came to me r...|\n",
            "| 72|    5.0|It doesn't make s...|\n",
            "| 79|    2.0|Purchased this ca...|\n",
            "| 88|    4.0|Two of three of t...|\n",
            "| 90|    5.0|My cat loves the ...|\n",
            "|103|    5.0|powerful protecti...|\n",
            "|105|    5.0|Power - Mune Chic...|\n",
            "|106|    5.0|power mune by vet...|\n",
            "|109|    5.0|Liver Support\" Su...|\n",
            "|123|    5.0|Excellent-so info...|\n",
            "|129|    1.0|Bad science. Bad ...|\n",
            "|139|    5.0|I Love Love Love ...|\n",
            "|142|    5.0|            Awesome!|\n",
            "|176|    5.0|Fast shipping.. L...|\n",
            "+---+-------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "distribution = review_df.groupBy('overall').count().collect()\n",
        "\n",
        "# Calculate the sampling ratios based on the distribution\n",
        "sampling_ratios = {row['overall']: 0.1 for row in distribution}\n",
        "\n",
        "# Use sampleBy to downsample the data\n",
        "review_df = review_df.sampleBy('overall', fractions=sampling_ratios, seed=42)\n",
        "\n",
        "# Show the downsampled DataFrame\n",
        "review_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYbq7RlonJ-2"
      },
      "source": [
        "## Analysis of Review Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M660z3AAnJ-2"
      },
      "source": [
        "### Visualuze count distribution of overall rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e-iVKzbnJ-2"
      },
      "outputs": [],
      "source": [
        "# convert columns id and overall from sampled_df to pd\n",
        "sampled_df_pd = review_df.select('overall').toPandas()\n",
        "# convert overall col to int\n",
        "sampled_df_pd['overall'] = sampled_df_pd['overall'].astype(int)\n",
        "\n",
        "plt.bar(sampled_df_pd['overall'].value_counts().index,\n",
        "        sampled_df_pd['overall'].value_counts().values,\n",
        "        color='darkseagreen')\n",
        "plt.title('Count of Overall Rating')\n",
        "plt.xlabel('Overall Rating')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3obYynjnJ-3"
      },
      "source": [
        "#### Process and clean reviewText\n",
        "* Make all words lowercase and remove punctuation\n",
        "* StopWordsRemover to remove words like [the, with, so, and...] as they are common words don't tell much about the text information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPvPBqmFnJ-3"
      },
      "outputs": [],
      "source": [
        "# remove punctuation from reviewText, convert to lowercase, split reviews into list of words\n",
        "clean_txt = review_df.withColumn(\"reviewText\", sf.regexp_replace(sf.col(\"reviewText\"), \"[^a-zA-Z0-9\\\\s]\", \"\")) \\\n",
        "                        .withColumn(\"reviewText\", sf.lower(sf.col(\"reviewText\"))) \\\n",
        "                        .withColumn(\"splitText\", sf.split(sf.col(\"reviewText\"), \" \"))\n",
        "\n",
        "# remove stop words [the, with, etc.]\n",
        "stop_words_remover = StopWordsRemover() \\\n",
        "                    .setInputCol(\"splitText\") \\\n",
        "                    .setOutputCol(\"filteredWords\") \\\n",
        "                    .transform(clean_txt)\n",
        "\n",
        "# apply stop_words_remover to reviewText column\n",
        "filtered_df = stop_words_remover.select('id', 'overall', 'reviewText', 'filteredWords')\n",
        "# print(filtered_df.show(5, truncate=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5e8IAXJnJ-3"
      },
      "outputs": [],
      "source": [
        "print(filtered_df.show(5, truncate=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1cQhf5WnJ-3"
      },
      "source": [
        "### Count word frequency and add to dictionary of {word: count} using rdd's"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LbEGjGI4nJ-3"
      },
      "outputs": [],
      "source": [
        "# Convert DataFrame to RDD and perform mapping and reduceByKey to create dict of {word : count}\n",
        "word_counts_rdd = (\n",
        "    filtered_df.select(\"id\", \"filteredWords\")\n",
        "    .rdd  # Convert to RDD for map-reduce\n",
        "    .flatMap(lambda row: [(word, 1) for word in row[\"filteredWords\"]])\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        ")\n",
        "\n",
        "# sorted word_counts_rdd by count\n",
        "word_counts_rdd = word_counts_rdd.sortBy(lambda x: x[1], ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qczf4IVdnJ-4"
      },
      "source": [
        "Add values to dictionary from sorted list, and sort by keys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzjHTX0SnJ-4"
      },
      "outputs": [],
      "source": [
        "# convert word_counts_rdd to dict\n",
        "word_counts_dict = dict(word_counts_rdd.collect())\n",
        "\n",
        "# remove all keys '' from word_counts_dict\n",
        "word_counts_dict = {k: v for k, v in word_counts_dict.items() if k != ''}\n",
        "\n",
        "top_20_words = dict(sorted(word_counts_dict.items(), key=lambda x: x[1], reverse=True)[:20])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfbBnOd0nJ-4"
      },
      "outputs": [],
      "source": [
        "print(top_20_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlTWgv48nJ-4"
      },
      "source": [
        "#### Add dictionary for each review and add to RDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U35c46hDnJ-5"
      },
      "outputs": [],
      "source": [
        "def count_words(words):\n",
        "    \"\"\"\n",
        "    Count the number of times each word appears in a list of words, for each review.\n",
        "    Args:\n",
        "        words (list): list of words\n",
        "    Returns:\n",
        "        dict: {word : counts}\n",
        "    \"\"\"\n",
        "    # remove newline characters\n",
        "    words = [word.replace('\\n', '') for word in words]\n",
        "    # Filter out empty strings\n",
        "    words = [word for word in words if word.strip()]\n",
        "    return {word: words.count(word) for word in set(words)}\n",
        "\n",
        "\n",
        "# add a new column with word counts as a dict\n",
        "def add_word_counts(row):\n",
        "    word_counts = count_words(row[\"filteredWords\"])\n",
        "    return (row[\"id\"], row[\"overall\"], word_counts)\n",
        "\n",
        "# apply function to each row in the original df\n",
        "filtered_df_with_counts_rdd = filtered_df.rdd.map(lambda row: add_word_counts(row))\n",
        "# print(filtered_df_with_counts_rdd.take(5))\n",
        "# # convert RDD back to df\n",
        "filtered_df_with_counts = filtered_df_with_counts_rdd.toDF([\"id\", \"overall\", \"word_counts\"])\n",
        "\n",
        "filtered_df_with_counts.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vmOaQ5onJ-5"
      },
      "source": [
        "#### Calculate average rating per review with each of the top 20 most common words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "notegPBInJ-5"
      },
      "outputs": [],
      "source": [
        "# Explode the word_counts column to separate rows\n",
        "df_exploded = filtered_df_with_counts.select(\"id\", \"overall\", sf.explode(\"word_counts\").alias(\"word\", \"count\"))\n",
        "# print(df_exploded.show(5, truncate=False))\n",
        "\n",
        "# Filter rows based on the list of words\n",
        "top_20_list = list(top_20_words.keys())\n",
        "df_filtered = df_exploded.filter(df_exploded.word.isin(top_20_list))\n",
        "\n",
        "# Calculate the sum of ratings and count of occurrences for each word\n",
        "df_avg_rating = df_filtered.groupBy(\"word\").agg(sf.sum('overall'), sf.sum(\"count\"))\n",
        "\n",
        "# sort df_avg_rating by count\n",
        "df_avg_rating = df_avg_rating.sort(\"sum(count)\", ascending=False)\n",
        "# print(df_avg_rating.show(5, truncate=False))\n",
        "\n",
        "# Calculate the average rating for each word\n",
        "df_avg_rating = df_avg_rating.withColumn(\"avg_rating\", df_avg_rating[\"sum(overall)\"] / df_avg_rating[\"sum(count)\"])\n",
        "\n",
        "# Show the result\n",
        "df_avg_rating.show(20, truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LKXxizenJ-5"
      },
      "source": [
        "#### Plot results of top 20 words for whole dataset along with average rating per word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1RyL7P1nJ-5"
      },
      "outputs": [],
      "source": [
        "# plot histogram of word\n",
        "df_plot = df_avg_rating.toPandas()\n",
        "fig, ax1 = plt.subplots()\n",
        "\n",
        "# axis 1 -> bar plot of word counts\n",
        "ax1.bar(df_plot['word'],\n",
        "        df_plot['sum(count)'],\n",
        "        color='darkseagreen')\n",
        "ax1.set_xlabel('Word')\n",
        "ax1.set_ylabel('Count')\n",
        "\n",
        "# ax2 -> line plot of average rating\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(df_plot['word'],\n",
        "         df_plot['avg_rating'],\n",
        "         color='royalblue',\n",
        "         marker='o',\n",
        "         label='Average Rating')\n",
        "ax2.set_ylabel('Average Rating')\n",
        "\n",
        "# Title for the plot\n",
        "plt.title('Word Counts and Average Rating')\n",
        "\n",
        "ticks = range(len(df_plot['word']))\n",
        "ax1.set_xticks(ticks)\n",
        "ax2.set_xticks(ticks)\n",
        "ax1.set_xticklabels(df_plot['word'], rotation=45, ha='right')\n",
        "ax2.set_xticklabels(df_plot['word'], rotation=45, ha='right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNrhHf6_nJ-6"
      },
      "source": [
        "### Plot most common words for each overall rating group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COr734lMnJ-6"
      },
      "outputs": [],
      "source": [
        "df_exploded = filtered_df_with_counts.select(\"id\", \"overall\", explode(\"word_counts\").alias(\"word\", \"count\"))\n",
        "overall_1 = df_exploded.filter(df_exploded.overall == 1.0) \\\n",
        "                        .groupBy(\"word\").agg(sum(\"overall\"), sum(\"count\")) \\\n",
        "                        .sort(\"sum(count)\", ascending=False)\n",
        "\n",
        "avg_rating_1 = overall_1.withColumn(\"avg_rating\", overall_1[\"sum(overall)\"] / overall_1[\"sum(count)\"]) \\\n",
        "                        .limit(10)\n",
        "\n",
        "# plot histogram of word\n",
        "overall_1 = avg_rating_1.toPandas()\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(overall_1['word'],\n",
        "        overall_1['sum(count)'],\n",
        "        color='darkseagreen')\n",
        "ax.set_xlabel('Word')\n",
        "ax.set_ylabel('Count')\n",
        "ax.xaxis.set_tick_params(rotation=45)\n",
        "# set y axis range to 30000\n",
        "ax.set_ylim([0, 120000])\n",
        "plt.title('Highest Word Counts for Overall Rating 1')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htkDwb3HnJ-6"
      },
      "outputs": [],
      "source": [
        "overall_2 = df_exploded.filter(df_exploded.overall == 2.0) \\\n",
        "                        .groupBy(\"word\").agg(sum(\"overall\"), sum(\"count\")) \\\n",
        "                        .sort(\"sum(count)\", ascending=False)\n",
        "\n",
        "avg_rating_2 = overall_2.withColumn(\"avg_rating\", overall_2[\"sum(overall)\"] / overall_2[\"sum(count)\"]) \\\n",
        "                        .limit(10)\n",
        "\n",
        "# plot histogram of word\n",
        "overall_2 = avg_rating_2.toPandas()\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(overall_2['word'],\n",
        "        overall_2['sum(count)'],\n",
        "        color='darkseagreen')\n",
        "ax.set_xlabel('Word')\n",
        "ax.set_ylabel('Count')\n",
        "ax.xaxis.set_tick_params(rotation=45)\n",
        "# set y axis range to 30000\n",
        "ax.set_ylim([0, 120000])\n",
        "plt.title('Highest Word Counts for Overall Rating 2')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6cBwGGNnJ-6"
      },
      "outputs": [],
      "source": [
        "overall_3 = df_exploded.filter(df_exploded.overall == 3.0) \\\n",
        "                        .groupBy(\"word\").agg(sum(\"overall\"), sum(\"count\")) \\\n",
        "                        .sort(\"sum(count)\", ascending=False)\n",
        "\n",
        "avg_rating_3 = overall_3.withColumn(\"avg_rating\", overall_3[\"sum(overall)\"] / overall_3[\"sum(count)\"]) \\\n",
        "                        .limit(10)\n",
        "\n",
        "# plot histogram of word\n",
        "overall_3 = avg_rating_3.toPandas()\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(overall_3['word'],\n",
        "        overall_3['sum(count)'],\n",
        "        color='darkseagreen')\n",
        "ax.set_xlabel('Word')\n",
        "ax.set_ylabel('Count')\n",
        "ax.xaxis.set_tick_params(rotation=45)\n",
        "# set y axis range to 30000\n",
        "ax.set_ylim([0, 120000])\n",
        "plt.title('Highest Word Counts for Overall Rating 3')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOVv7QPgnJ-7"
      },
      "outputs": [],
      "source": [
        "overall_4 = df_exploded.filter(df_exploded.overall == 4.0) \\\n",
        "                        .groupBy(\"word\").agg(sum(\"overall\"), sum(\"count\")) \\\n",
        "                        .sort(\"sum(count)\", ascending=False)\n",
        "\n",
        "avg_rating_4 = overall_4.withColumn(\"avg_rating\", overall_4[\"sum(overall)\"] / overall_4[\"sum(count)\"]) \\\n",
        "                        .limit(10)\n",
        "\n",
        "# plot histogram of word\n",
        "overall_4 = avg_rating_4.toPandas()\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(overall_4['word'],\n",
        "        overall_4['sum(count)'],\n",
        "        color='darkseagreen')\n",
        "ax.set_xlabel('Word')\n",
        "ax.set_ylabel('Count')\n",
        "ax.xaxis.set_tick_params(rotation=45)\n",
        "# set y axis range to 30000\n",
        "ax.set_ylim([0, 120000])\n",
        "plt.title('Highest Word Counts for Overall Rating 4')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJnknfzqnJ-7"
      },
      "outputs": [],
      "source": [
        "overall_5 = df_exploded.filter(df_exploded.overall == 5.0) \\\n",
        "                        .groupBy(\"word\").agg(sum(\"overall\"), sum(\"count\")) \\\n",
        "                        .sort(\"sum(count)\", ascending=False)\n",
        "\n",
        "avg_rating_5 = overall_5.withColumn(\"avg_rating\", overall_5[\"sum(overall)\"] / overall_5[\"sum(count)\"]) \\\n",
        "                        .limit(10)\n",
        "\n",
        "# plot histogram of word\n",
        "overall_5 = avg_rating_5.toPandas()\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(overall_5['word'],\n",
        "        overall_5['sum(count)'],\n",
        "        color='darkseagreen')\n",
        "ax.set_xlabel('Word')\n",
        "ax.set_ylabel('Count')\n",
        "ax.xaxis.set_tick_params(rotation=45)\n",
        "# set y axis range to 30000\n",
        "# ax.set_ylim([0, 120000])\n",
        "plt.title('Highest Word Counts for Overall Rating 5')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cVBs__jnJ-7"
      },
      "source": [
        "# Metadata for Pet Supplies Products"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV_tPhzrnJ-8"
      },
      "outputs": [],
      "source": [
        "meta_data = spark.read.json(\"/content/drive/MyDrive/Big_Data/Data/meta_Pet_Supplies.json\")\n",
        "# meta_data = spark.read.json(path='/content/drive/MyDrive/Big_Data/Data/Pet_Supplies.json', schema=meta_schema)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtKsEngtnJ-9"
      },
      "source": [
        "Check for null values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9G03foKnJ-9"
      },
      "outputs": [],
      "source": [
        "# check null values\n",
        "print(f\"Null asin: {meta_data.filter(sf.col('asin').isNull()).count()}\")\n",
        "print(f\"Null also_buy: {meta_data.filter(sf.col('also_buy').isNull()).count()}\")\n",
        "print(f\"Null also_view: {meta_data.filter(sf.col('also_view').isNull()).count()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iPBplmSXnJ-9"
      },
      "outputs": [],
      "source": [
        "# select asin, also_buy, also_view from meta_df where also_view and also_buy are not null\n",
        "meta = meta_data.select('asin', 'also_buy', 'also_view')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gwfxzyT5nJ-9"
      },
      "outputs": [],
      "source": [
        "# # randomly sample 5000 rows from meta_data for testing\n",
        "# meta = meta.sample(False, 0.5, seed=0) # df\n",
        "# print(f\"Shape of meta: ({meta.count()}, {len(meta.columns)})\") # (5,000, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN3M4QdsnJ--"
      },
      "source": [
        "### RDD of meta data and identify duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmOG5y4bnJ--"
      },
      "outputs": [],
      "source": [
        "# create rdd of view_data using the spark context and parallelize\n",
        "view_rdd = sc.parallelize(meta.collect())\n",
        "\n",
        "# print rows with count asin > 1 (duplicates)\n",
        "dupes = view_rdd.map(lambda x: (x[0], 1)) \\\n",
        "                .reduceByKey(lambda x, y: x + y) \\\n",
        "                .filter(lambda x: x[1] > 1).collect()\n",
        "# get only the asin\n",
        "dupes = [x[0] for x in dupes]\n",
        "print(dupes) # 6\n",
        "# remove duplicate asin from view_rdd\n",
        "view_rdd = view_rdd.filter(lambda x: x[0] not in dupes)\n",
        "# print(f\"Shape of view_rdd after removing {len(dupes)} duplicate values: ({view_rdd.count()}, {len(view_rdd.take(1)[0])})\") # (4,994, 3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9OUYPNBnJ--"
      },
      "outputs": [],
      "source": [
        "# check amount of unique asin\n",
        "print(f\"Count unique asin: {view_rdd.map(lambda x: x[0]).distinct().count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01DZVb_BnJ--"
      },
      "source": [
        "### Joining Product Metadata and Review Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xq9o6HGMnJ--",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "025e98ac-d97a-41f5-8084-fed2e37a795e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+--------------+--------------------+--------------------+\n",
            "|      asin|    reviewerID|            also_buy|           also_view|\n",
            "+----------+--------------+--------------------+--------------------+\n",
            "|0615553605| AL933I7VQRKBZ|                  []|[B075DYQ1PH, 1604...|\n",
            "|0615553605| AWTI98YZPFQTH|                  []|[B075DYQ1PH, 1604...|\n",
            "|0615553605|A1NW1C0J0VYG0A|                  []|[B075DYQ1PH, 1604...|\n",
            "|0615553605|A3OMJBE0MF1721|                  []|[B075DYQ1PH, 1604...|\n",
            "|0615553605| A1SDED4QSDX4I|                  []|[B075DYQ1PH, 1604...|\n",
            "|0615553605| ALLFL71KWYWWP|                  []|[B075DYQ1PH, 1604...|\n",
            "|0615553605|  ABZ8CQXD42H4|                  []|[B075DYQ1PH, 1604...|\n",
            "|0615553605|A363P047LR5XI6|                  []|[B075DYQ1PH, 1604...|\n",
            "|0615553605|A3PG0KS1YE8MR4|                  []|[B075DYQ1PH, 1604...|\n",
            "|0793816793| AD7AYZY8UPDTG|[1911142186, 1514...|                  []|\n",
            "|0793816793|A2I21DHTEPWWMU|[1911142186, 1514...|                  []|\n",
            "|0793816793|A1L0VY8EUL5AW3|[1911142186, 1514...|                  []|\n",
            "|0793816793|A3OADEZTJAOM3K|[1911142186, 1514...|                  []|\n",
            "|0793816793|A2R1QGZJUQNUOI|[1911142186, 1514...|                  []|\n",
            "|0793816793|A3QNRH7X5G0RJK|[1911142186, 1514...|                  []|\n",
            "|0972585419|A13K4OZKAAHOXS|[B0002FP328, B000...|[B0002FP328, B00C...|\n",
            "|0972585419|A1DWYEX4P7GB7Z|[B0002FP328, B000...|[B0002FP328, B00C...|\n",
            "|0972585419|A3NVN97YJSKEPC|[B0002FP328, B000...|[B0002FP328, B00C...|\n",
            "|0972585419|A1PDMES1LYA0DP|[B0002FP328, B000...|[B0002FP328, B00C...|\n",
            "|0972585419| AT6BH0TQLZS5X|[B0002FP328, B000...|[B0002FP328, B00C...|\n",
            "+----------+--------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "joined_df = data_df.select('asin', 'reviewerID') \\\n",
        "                    .join(meta.select('asin', 'also_buy', 'also_view'),on='asin', how='inner') \\\n",
        "                    .filter(sf.col('reviewerID').isNotNull())\n",
        "joined_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pc3Vkbx8nJ-_"
      },
      "outputs": [],
      "source": [
        "# count distinct also_buy and asin -> check if also_buy corresponds exactly to asin\n",
        "print(f\"Count distinct also_buy: {joined_df.select('also_buy').distinct().count()}\")\n",
        "print(f\"Count distinct asin: {joined_df.select('asin').distinct().count()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOkuZQddnJ-_"
      },
      "source": [
        "## What percent of similar purchased items are purchased?\n",
        "## What percent of viewed items are purchased?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iOsFTMoknJ-_"
      },
      "outputs": [],
      "source": [
        "# find where also_buy and also_view are not empty\n",
        "filter_buy = joined_df.filter(sf.size(sf.col('also_buy')) > 0) \\\n",
        "                        .filter(sf.size(sf.col('also_view')) > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFHTgwK1nJ-_"
      },
      "source": [
        "Create lists of what each reviewer bought and viewed\n",
        " 1. group by reviewerID\n",
        " 2. create list of all asin with that reviewerID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "HK99yxzgnJ-_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f13b857-802b-4cc7-e69b-0f6e08bf62d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+----------+--------------------+--------------------+--------------------+\n",
            "|          reviewerID|      asin|     reviewer_bought|            also_buy|           also_view|\n",
            "+--------------------+----------+--------------------+--------------------+--------------------+\n",
            "|A0020320YVA26ZNLOUX3|B0009YYSBE|[B01DMTPS8E, B000...|[B004ZM65LC, B000...|[B000HCI5CG, B01B...|\n",
            "|A0020320YVA26ZNLOUX3|B01DMTPS8E|[B01DMTPS8E, B000...|[B01M4OPTQD, B071...|[B007P54O0A, B00F...|\n",
            "|A0084055RM28SR8A0DQM|B004ZM7JB2|        [B004ZM7JB2]|[B003JHLY1U, B004...|[B00ODRO7W8, B00F...|\n",
            "|A0207285Q798IKKBY9TI|B011AY4JWO|        [B011AY4JWO]|[B002B8WJPI, B01E...|[B07DGZJX6B, B07H...|\n",
            "|A0231053UDFCC75M0RI3|B00VE78ZPU|        [B00VE78ZPU]|[B0050ICKN2, B005...|[B075Q57QGL, B07B...|\n",
            "|A02430977ZM6HTS5PYW2|B00AQU8F2O|        [B00AQU8F2O]|[B00AQU8HAO, B00A...|[B00AQU8HAO, B00A...|\n",
            "|A02626447PIKHLBV4CJY|B005F5DHW8|        [B005F5DHW8]|[B002H3BLCY, B003...|[B0002DJ4P2, B001...|\n",
            "|A0314454CJYQE4NT3D8H|B013Q7FGMA|        [B013Q7FGMA]|[B00EZMPD4W, B06V...|[B017K2GPOK, B00K...|\n",
            "|A0354137YAOEVA07ZWQD|B0002DIPRK|[B0002DIPRK, B000...|        [B00I2Y1CJY]|[B01KVWIKHG, B018...|\n",
            "|A0354137YAOEVA07ZWQD|B0002DIPRK|[B0002DIPRK, B000...|        [B00I2Y1CJY]|[B01KVWIKHG, B018...|\n",
            "|A03622674FSWUX0PXPAK|B00MM6ZIF2|[B00C6UXMKW, B00M...|[B00BP5H71O, B018...|[B01K4H2ZVU, B01N...|\n",
            "|A03622674FSWUX0PXPAK|B00OLSBLHS|[B00C6UXMKW, B00M...|[B00OLSBMAE, B00O...|[B00OLSBP9C, B01N...|\n",
            "|A03622674FSWUX0PXPAK|B00C6UXMKW|[B00C6UXMKW, B00M...|        [B00CJQ3VJA]|[B00CJQ3VJA, B002...|\n",
            "|A0382115G8ABVFE2S38K|B0009EUZWA|        [B0009EUZWA]|[B0041QFI06, B006...|[B006NOO440, B01B...|\n",
            "|A0475801YABHZEXI59M4|B002CKAJYS|        [B002CKAJYS]|[B06VXQQKRG, B00I...|[B06VXQQKRG, B006...|\n",
            "|A05134716IVB6LMBQLHV|B0002ARYY8|[B0002ARYY8, B000...|[B00HKK6HOQ, B003...|[B00HYF6198, B009...|\n",
            "|A05134716IVB6LMBQLHV|B0002ARYY8|[B0002ARYY8, B000...|[B00HKK6HOQ, B003...|[B00HYF6198, B009...|\n",
            "|A0552410LJL93P6YG07B|B006ZNGZLY|        [B006ZNGZLY]|[B009X13LNK, B007...|[B00QW83U9C, B077...|\n",
            "|A063166735IQ7W9BUB05|B00005OMWQ|[B00005OMWQ, B000...|[B00014TQWI, B001...|[B0012BUI4E, B007...|\n",
            "|A063166735IQ7W9BUB05|B00005OMWQ|[B00005OMWQ, B000...|[B00014TQWI, B001...|[B0012BUI4E, B007...|\n",
            "+--------------------+----------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# group by reviewerID and create new column with list of asin\n",
        "grouped_reviewer = filter_buy.groupBy('reviewerID') \\\n",
        "                        .agg(sf.collect_list('asin').alias('reviewer_bought'))\n",
        "\n",
        "grouped_reviewer = filter_buy.join(grouped_reviewer, on='reviewerID', how='outer').select('reviewerID', 'asin', 'reviewer_bought', 'also_buy', 'also_view')\n",
        "grouped_reviewer.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AICU-U79nJ_A"
      },
      "outputs": [],
      "source": [
        "# sample group_reviewer\n",
        "# sample_grouped_reviewer = grouped_reviewer.sample(False, 0.005, seed=0)\n",
        "# # combine also_buy and also_view into their own columns for that reviewerID\n",
        "# combined_df = sample_grouped_reviewer.agg(sf.flatten(sf.collect_list(\"also_buy\")),\n",
        "#                                                 sf.flatten(sf.collect_list(\"also_view\")))\n",
        "# combined_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZF7Zo_PSnJ_A"
      },
      "outputs": [],
      "source": [
        "# calculating how many items in also_buy are in also_view (intersection)\n",
        "buy_also_bought = grouped_reviewer.withColumn(\"intersect_bought_count\", sf.size(sf.array_intersect(\"also_buy\", \"reviewer_bought\"))) \\\n",
        "                                        .sort(sf.desc(\"intersect_bought_count\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lzgjYrrlnJ_A"
      },
      "outputs": [],
      "source": [
        "buy_viewed = buy_also_bought.withColumn(\"intersect_view_count\", sf.size(sf.array_intersect(\"also_view\", \"reviewer_bought\"))) \\\n",
        "                                        .sort(sf.desc(\"intersect_view_count\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "dJiuNDB5nJ_A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fc425bd-2e86-409b-c270-9b198fdcb87f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+----------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------+------------+\n",
            "|    reviewerID|      asin|     reviewer_bought|            also_buy|           also_view|intersect_bought_count|intersect_view_count|percent_buy|percent_view|\n",
            "+--------------+----------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------+------------+\n",
            "|A3V0D97QKXDN5R|B002CJCEQA|[B000WTEJC4, B001...|[B002CJG1HI, B001...|[B002CJARSW, B001...|                    10|                   9|      55.56|        50.0|\n",
            "|A3F22SXQNXWV3K|B007CLZ3HA|[B000084F1Z, B000...|[B007CLZRWG, B00C...|[B007CLZRWG, B00C...|                     7|                   8|       20.0|       22.86|\n",
            "|A3V0D97QKXDN5R|B002CJG1YQ|[B000WTEJC4, B001...|[B002CJAS86, B002...|[B002CJARSW, B002...|                    10|                   8|      55.56|       44.44|\n",
            "|A3F22SXQNXWV3K|B007CLZRWG|[B000084F1Z, B000...|[B00CHJ6JQ6, B007...|[B00CHJ6JQ6, B007...|                     7|                   8|       20.0|       22.86|\n",
            "|A3F22SXQNXWV3K|B000MT4HWG|[B000084F1Z, B000...|[B000NVBVIQ, B007...|[B007CLZRWG, B00C...|                     7|                   8|       20.0|       22.86|\n",
            "|A3F22SXQNXWV3K|B000NVBVIQ|[B000084F1Z, B000...|[B000MT4HWG, B007...|[B007CLZRWG, B007...|                     7|                   8|       20.0|       22.86|\n",
            "|A3V0D97QKXDN5R|B002CJG1HI|[B000WTEJC4, B001...|[B002CJASQ8, B002...|[B001G0NKZA, B002...|                     9|                   7|       50.0|       38.89|\n",
            "|A3F22SXQNXWV3K|B0057LBU60|[B000084F1Z, B000...|[B0057LBU5Q, B005...|[B000MT4HWG, B005...|                     7|                   7|       20.0|        20.0|\n",
            "|A3V0D97QKXDN5R|B000WTEJC4|[B000WTEJC4, B001...|[B002CJCEQA, B002...|[B005FGPWYS, B01G...|                     4|                   7|      22.22|       38.89|\n",
            "|A3F22SXQNXWV3K|B00CHJ6JQ6|[B000084F1Z, B000...|[B007CLZRWG, B007...|[B007CLZRWG, B000...|                     7|                   7|       20.0|        20.0|\n",
            "|A3V0D97QKXDN5R|B000WTEJC4|[B000WTEJC4, B001...|[B002CJCEQA, B002...|[B005FGPWYS, B01G...|                     4|                   7|      22.22|       38.89|\n",
            "|A3F22SXQNXWV3K|B00TFMS9SY|[B000084F1Z, B000...|[B00UHJEGKE, B01F...|[B00UHJEGKE, B007...|                     8|                   7|      22.86|        20.0|\n",
            "|A3V0D97QKXDN5R|B002CJG1BY|[B000WTEJC4, B001...|[B002CJG29U, B002...|[B00HMZDFHQ, B00H...|                     9|                   7|       50.0|       38.89|\n",
            "|A3F22SXQNXWV3K|B00U5R217I|[B000084F1Z, B000...|[B00UFCOEK0, B01F...|[B00UFCOEK0, B00T...|                     4|                   7|      11.43|        20.0|\n",
            "|A3F22SXQNXWV3K|B001J8H1E0|[B000084F1Z, B000...|[B000MT4HWG, B007...|[B007CLZRWG, B000...|                     7|                   7|       20.0|        20.0|\n",
            "|A3IRS1AY45IWXR|B001L1197A|[B001J8H1E0, B005...|[B007CLZRWG, B001...|[B007CLZRWG, B001...|                     6|                   6|      85.71|       85.71|\n",
            "|A3GFJ2GBY6779L|B00O0HM1M4|[B0002AR15U, B000...|[B0000AH3UC, B000...|[B00TWNP4R0, B009...|                     4|                   6|      16.67|        25.0|\n",
            "|A24G85HUDLJ8JA|B004INIUQQ|[B0009JQRHC, B000...|[B002CJG13C, B002...|[B002CJG1TG, B002...|                     8|                   6|       50.0|        37.5|\n",
            "|A24G85HUDLJ8JA|B002CJAS86|[B0009JQRHC, B000...|[B002CJASXQ, B004...|[B00HMZDFJO, B002...|                     8|                   6|       50.0|        37.5|\n",
            "|A24G85HUDLJ8JA|B0085JN424|[B0009JQRHC, B000...|[B001G0NKVO, B002...|[B002CJG1TG, B077...|                     8|                   6|       50.0|        37.5|\n",
            "+--------------+----------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# calculate percentages of bought items that are actually bought, and viewed items that are actually bought\n",
        "prob_buy = buy_viewed.withColumn(\"percent_buy\", sf.round(sf.col(\"intersect_bought_count\") / sf.size(sf.col(\"reviewer_bought\")) * 100,2)) \\\n",
        "                        .withColumn(\"percent_view\", sf.round(sf.col(\"intersect_view_count\") / sf.size(sf.col(\"reviewer_bought\")) * 100,2))\n",
        "\n",
        "prob_buy.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "avg_prob_buy = prob_buy.agg(sf.avg(\"percent_buy\")).collect()[0][0]\n",
        "avg_prob_view = prob_buy.agg(sf.avg(\"percent_view\")).collect()[0][0]"
      ],
      "metadata": {
        "id": "92zZCqTpuF5c"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(avg_prob_buy)\n",
        "print(avg_prob_view)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkGpYwXNvMFx",
        "outputId": "79d1f08c-8bc3-460d-e192-c62704c19864"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9374184898625909\n",
            "0.8264738793246708\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zQ8HUhYFCX2k"
      ],
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}